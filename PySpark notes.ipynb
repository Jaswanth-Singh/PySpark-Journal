{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62980b96",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "PySpark is a Python API which is an analytical processing engine for large-scale powerful distributed data processing and machine learning applications.\n",
    "100 times faster than traditional systems and can be used to process real time data.\n",
    "It natively has machine learning and graph libraries.\n",
    "Supports Python v3.8 or newer\n",
    "\n",
    "# Apache Spark\n",
    "\n",
    "Has a master(driver) slave(worker) architecture. When a spark application is run, the driver program creates a context which serves as an entry point to the application. The cluster manager is responsible for managing the resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ba481",
   "metadata": {},
   "source": [
    "## PySpark Modules and packages\n",
    "\n",
    "- PySpark RDD (pyspark.RDD)\n",
    "- PySpark DataFrame and SQL (pyspark.sql)\n",
    "- PySpark Streaming (pyspark.streaming)\n",
    "- PySpark MLib (pyspark.ml, pyspark.mllib) \n",
    "- PySpark GraphFrames (GraphFrames)\n",
    "- PySpark Resource (pyspark.resource) Itâ€™s new in PySpark 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0682a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import py4j\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6f2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SarkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bafc92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "\n",
    "#parallelize method is used to crearte an RDD from a list\n",
    "rdd=spark.sparkContext.parallelize(dataList)\n",
    "\n",
    "#to create an RDD from a text file\n",
    "#rdd = spark.sparkContext.textFile(\"file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03878264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92078693",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "PySpark RDD (Resilient Distributed Dataset) is a fundamental data structure of PySpark that is fault-tolerant, immutable distributed collections of objects, which means once you create an RDD you cannot change it. Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster.\n",
    "\n",
    "To create an rdd, it is required to create a spark session.\n",
    "\n",
    "    Note: There can be multiple spark sessions but only one spark context in a jvm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c0408",
   "metadata": {},
   "source": [
    "## RDD operations\n",
    "\n",
    "### RDD transformations - \n",
    "Transformations on an RDD return a new RDD with the updates made and the original RDD remains unchanged. These operations do not execute until we call an action on an RDD(lazy operations).\n",
    "    Examples: map(), reduceByKey(), filter(), sortByKey(), flatMap()\n",
    "\n",
    "### RDD actions -\n",
    "Operations that return a non RDD value to the driver node.\n",
    "    Examples: count(), collect(), reduce(), max(), first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd528ce2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c2245bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[2] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "#Creating and Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD() #alternative   emptyRDD = spark.sparkContext.parallelize([])\n",
    "print(emptyRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb009570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a schema\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "schema = StructType([\n",
    "  StructField('firstname', StringType(), True),\n",
    "  StructField('middlename', StringType(), True),\n",
    "  StructField('lastname', StringType(), True)\n",
    "  ])\n",
    "\n",
    "#Creating an empty DataFrame without RDD\n",
    "#df = spark.createDataFrame([],schema)\n",
    "\n",
    "#Creating an empty DataFrame from an empty RDD\n",
    "df = spark.createDataFrame(emptyRDD,schema)  #alternative    df1 = emptyRDD.toDF(schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25fe18e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating an empty DataFrame without schema\n",
    "emptyDf = spark.createDataFrame([], StructType([]))\n",
    "emptyDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbcba0d",
   "metadata": {},
   "source": [
    "## PySpark DataFrame to Pandas DataFrame\n",
    "PySpark is faster than Pandas because it runs on multiple nodes in a parallel fashion whereas pandas only runs on a single machine. Processing large datasets becomes efficient using PySpark. After processing, we might need to convert the PySpark DataFrame to Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581fda17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+\n",
      "|first_name|middle_name|last_name|\n",
      "+----------+-----------+---------+\n",
      "|John      |           |Williams |\n",
      "|Karthick  |           |K        |\n",
      "|Karthik   |           |R        |\n",
      "+----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\",\"\",\"Williams\"),\n",
    "        (\"Karthick\",\"\",\"K\",),\n",
    "        (\"Karthik\",\"\",\"R\"),\n",
    "        ]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f51ffae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name middle_name last_name\n",
      "0       John              Williams\n",
      "1   Karthick                     K\n",
      "2    Karthik                     R\n"
     ]
    }
   ],
   "source": [
    "pandasDF = pysparkDF.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511f5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " first_name  | John     \n",
      " middle_name |          \n",
      " last_name   | Williams \n",
      "-RECORD 1---------------\n",
      " first_name  | Karthick \n",
      " middle_name |          \n",
      " last_name   | K        \n",
      "-RECORD 2---------------\n",
      " first_name  | Karthik  \n",
      " middle_name |          \n",
      " last_name   | R        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show method of DataFrames\n",
    "\n",
    "#dataFrame.show(numberOfRows, truncate, vertical)\n",
    "\n",
    "#numberOfRows is set to 20 by default and vertical is set to False which means columns are displayed horizontally\n",
    "#truncate is set to 20 by default(It only displays 20 characters of the column value)\n",
    "\n",
    "pysparkDF.show(truncate=False, vertical=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c484d",
   "metadata": {},
   "source": [
    "## StructType\n",
    "StructType is commonly used to define the scehma for creating a DataFrame. Using StructType and StructFields, we can create complex schemas with nested structuress. Using StructType schema ensures that the data is correctly interpreted and structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f1a7d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3100  |\n",
      "|{Michael, Rose, }   |40288|M     |4300  |\n",
      "|{Robert, , Williams}|42114|M     |1400  |\n",
      "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining schema using nested StructType\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, MapType\n",
    "\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f874920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- Other: struct (nullable = false)\n",
      " |    |-- Id: string (nullable = true)\n",
      " |    |-- Gender: string (nullable = true)\n",
      " |    |-- Salary: integer (nullable = true)\n",
      " |    |-- Grade: string (nullable = false)\n",
      "\n",
      "+--------------------+------------------------+\n",
      "|name                |Other                   |\n",
      "+--------------------+------------------------+\n",
      "|{James, , Smith}    |{36636, M, 3100, Medium}|\n",
      "|{Michael, Rose, }   |{40288, M, 4300, High}  |\n",
      "|{Robert, , Williams}|{42114, M, 1400, Low}   |\n",
      "|{Maria, Anne, Jones}|{39192, F, 5500, High}  |\n",
      "|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adding and changing columns\n",
    "from pyspark.sql.functions import col,struct,when\n",
    "#adds column \"Other\"\n",
    "updatedDF = df.withColumn(\"Other\", \n",
    "    struct(col(\"id\").alias(\"Id\"),\n",
    "    col(\"gender\").alias(\"Gender\"),\n",
    "    col(\"salary\").alias(\"Salary\"),\n",
    "    when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n",
    "      .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
    "      .otherwise(\"High\").alias(\"Grade\")\n",
    "  )).drop(\"id\",\"gender\",\"salary\")    #removes the columns id, gender and salary\n",
    "\n",
    "updatedDF.printSchema()\n",
    "updatedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d4b8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating schema from json\n",
    "\n",
    "import json\n",
    "schemaFromJson = StructType.fromJson(json.loads(structureSchema.json()))\n",
    "df = spark.createDataFrame(\n",
    "        spark.sparkContext.parallelize(structureData),schemaFromJson)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709f9d9",
   "metadata": {},
   "source": [
    "## Column class in pyspark \n",
    "### (pyspark.sql.Column)\n",
    "\n",
    "#### Accessing columns in data frame\n",
    "col(\"column_name\")<br>\n",
    "df.column_name<br>\n",
    "df[\"column_name\"]<br>\n",
    "\n",
    "\n",
    "#### Commonly used methods\n",
    "<b>alias()</b> - assigns an alias to the column<br>\n",
    "<b>asc()</b> and <b>desc()</b> - returns ascending and descending order of the column respectively<br>\n",
    "<b>cast()</b> and <b>astype()</b> - used to convert the column value to another data type<br>\n",
    "<b>between()</b> - returns a boolean expression when a column value is in between the lower and upper bound<br>\n",
    "<b>contains()</b> - see if a column value contains the substring.<br>\n",
    "<b>when</b> and <b>otherwise</b> - used like sql case expressions(see code below)<br>\n",
    "<b>getField()</b> - to get value by key in MapType fields or Struct.<br>\n",
    "<b>getItem()</b> - to get value by index or key in ArrayType and MapType StructFields<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "367d7ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+\n",
      "|fname| lname|   new_gender|\n",
      "+-----+------+-------------+\n",
      "|James|  Bond|Not Specified|\n",
      "| Anne|  Hill|       Female|\n",
      "|  Tom|Cruise|            T|\n",
      "|  Tom| Hardy|         Male|\n",
      "+-----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",\"Bond\",\"100\",''),\n",
    "      (\"Anne\",\"Hill\",\"200\",'F'),\n",
    "      (\"Tom\",\"Cruise\",\"400\",'T'),\n",
    "      (\"Tom\",\"Hardy\",\"400\",'M')] \n",
    "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
    "              .when(df.gender==\"F\",\"Female\") \\\n",
    "              .when(df.gender=='' ,\"Not Specified\") \\\n",
    "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7ccd617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|properties[hair]|\n",
      "+----------------+\n",
      "|           black|\n",
      "|           brown|\n",
      "|             red|\n",
      "|           black|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
    "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
    "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
    "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "            StructField('fname', StringType(), True),\n",
    "            StructField('lname', StringType(), True)])),\n",
    "        StructField('languages', ArrayType(StringType()),True),\n",
    "        StructField('properties', MapType(StringType(),StringType()),True)\n",
    "     ])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "\n",
    "df.select(df.properties.getField(\"hair\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75254f25",
   "metadata": {},
   "source": [
    "## DataFrame Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dafc76",
   "metadata": {},
   "source": [
    "#### select\n",
    "It is a transformation which returns a new DataFrame with the selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05860fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "|Tom Cruise|\n",
      "| Tom Brand|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select (selecting columns from a DataFrame)\n",
    "\n",
    "#select a single column\n",
    "#df.select(col(\"name\")).show()\n",
    "\n",
    "#select multiple columns\n",
    "#df.select(col(\"name\"),col(\"languages\")).show()\n",
    "\n",
    "#select all columns from a list\n",
    "#df.select([col for col in df.columns]).show()\n",
    "\n",
    "#select specific column from nested struct\n",
    "df.select(\"name.fname\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a382828",
   "metadata": {},
   "source": [
    "#### collect\n",
    "It is an action that returns the entire dataset in an array to the driver.<br>\n",
    "It returns an Array of Row type objects.<br>\n",
    "Calling collect on large datasets can lead to OutOfMemory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f3b098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "\n",
    "deptDetails = deptDF.collect()\n",
    "\n",
    "print(deptDetails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ebac5",
   "metadata": {},
   "source": [
    "#### WithColumn\n",
    "It is a transformation that returns a new DataFrame. It can be used to update the values of a column, create a new column, change data type of column, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb9f2826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n",
      "+---------+-------+--------+\n",
      "|dept_name|dept_id|Employee|\n",
      "+---------+-------+--------+\n",
      "|  Finance|     10|    true|\n",
      "|Marketing|     20|    true|\n",
      "|    Sales|     30|    true|\n",
      "|       IT|     40|    true|\n",
      "+---------+-------+--------+\n",
      "\n",
      "+----------+-------+\n",
      "|department|dept_id|\n",
      "+----------+-------+\n",
      "|   Finance|     10|\n",
      "| Marketing|     20|\n",
      "|     Sales|     30|\n",
      "|        IT|     40|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Changing data type\n",
    "deptDF.withColumn(\"dept_id\",col(\"dept_id\").cast(\"String\")).show()\n",
    "\n",
    "#Adding a column\n",
    "deptDF.withColumn(\"Employee\",lit(True)).show()\n",
    "\n",
    "#Renaming a column\n",
    "deptDF.withColumnRenamed(\"dept_name\",\"department\").show()\n",
    "\n",
    "#To remove a column\n",
    "#deptDF.drop(\"dept_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1188ee4c",
   "metadata": {},
   "source": [
    "#### filter\n",
    "It is used to select/filter rows in a DataFrame based on a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fbc7942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "+---------+-------+\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "+---------+-------+\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter using column value\n",
    "deptDF.filter(deptDF.dept_name == \"Finance\").show()\n",
    "\n",
    "#filter using sql expression\n",
    "deptDF.filter(\"dept_name == 'Finance'\").show()\n",
    "\n",
    "#filter using multiple conditions\n",
    "deptDF.filter((deptDF.dept_name==\"Finance\") | (deptDF.dept_name==\"IT\")).show()\n",
    "\n",
    "#filter using list values\n",
    "accepted_depts = [\"Finance\",\"IT\"]\n",
    "deptDF.filter(deptDF.dept_name.isin(accepted_depts)).show()\n",
    "\n",
    "#syntax for filter using array_contains\n",
    "#from pyspark.sql.functions import array_contains\n",
    "#df.filter(array_contains(df.col_name,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b2dc2",
   "metadata": {},
   "source": [
    "#### distinct() and dropDuplicates()\n",
    "<b>distinct</b> - can be used to return a new DataFrame with only the distinct rows.<br>\n",
    "<b>dropDuplicates</b> - returns a new DataFrame with the duplicate rows dropped, same as distinct. But dropDuplicates also takes column names as parameters that it should check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c80bef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using distinct: \n",
      "Distinct count:  9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|          Jen|   Finance|  3900|\n",
      "|      Michael|     Sales|  4600|\n",
      "|        Scott|   Finance|  3300|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|        James|     Sales|  3000|\n",
      "|       Robert|     Sales|  4100|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|         Saif|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "+-------------+----------+------+\n",
      "\n",
      "Using dropDuplicates: \n",
      "Distinct count:  9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|          Jen|   Finance|  3900|\n",
      "|      Michael|     Sales|  4600|\n",
      "|        Scott|   Finance|  3300|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|        James|     Sales|  3000|\n",
      "|       Robert|     Sales|  4100|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|         Saif|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "+-------------+----------+------+\n",
      "\n",
      "Using dropDuplicates with column parameters: \n",
      "Distinct count:  8\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        Maria|   Finance|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|       Robert|     Sales|  4100|\n",
      "|      Michael|     Sales|  4600|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "\n",
    "#using distinct\n",
    "print(\"Using distinct: \")\n",
    "distinctDF = df.distinct()\n",
    "print(\"Distinct count: \",str(distinctDF.count()))\n",
    "distinctDF.show()\n",
    "\n",
    "#using dropDuplicates\n",
    "print(\"Using dropDuplicates: \")\n",
    "distinctDF = df.dropDuplicates()\n",
    "print(\"Distinct count: \",str(distinctDF.count()))\n",
    "distinctDF.show()\n",
    "\n",
    "#using dropDuplicates with column parameters\n",
    "print(\"Using dropDuplicates with column parameters: \")\n",
    "distinctDF = df.dropDuplicates([\"department\",\"salary\"])\n",
    "print(\"Distinct count: \",str(distinctDF.count()))\n",
    "distinctDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39236fc9",
   "metadata": {},
   "source": [
    "#### sort and orderBy\n",
    "These methods are used for sorting the rows of the data frame based on one or more columns. By default they are sorted in ascending order. We can use .asc() and .desc() on column parameters to change the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbf4c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "#sorting based on department and state in ascending order(by default)\n",
    "df.sort(\"department\",\"state\").show(truncate=False)\n",
    "\n",
    "#sorting based on department in descending order\n",
    "df.orderBy(col(\"department\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088ed59",
   "metadata": {},
   "source": [
    "#### groupBy\n",
    "Used to group rows based on one or more column values. Often used in conjunction with aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9b636f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "|Marketing |171000    |85500.0          |39000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum,avg,max\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "\n",
    "#sum of salary of employees in each dept\n",
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)\n",
    "\n",
    "#having multiple aggregate functions\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3bd9a",
   "metadata": {},
   "source": [
    "#### join\n",
    "This operation is similar to join operation in SQL\n",
    "\n",
    "#### different joins\n",
    "<b>Inner Join:</b> Returns only the rows with matching keys in both DataFrames.<br>\n",
    "<b>Left Join:</b> Returns all rows from the left DataFrame and matching rows from the right DataFrame.<br>\n",
    "<b>Right Join:</b> Returns all rows from the right DataFrame and matching rows from the left DataFrame.<br>\n",
    "<b>Full Outer Join:</b> Returns all rows from both DataFrames, including matching and non-matching rows.<br>\n",
    "<b>Left Semi Join:</b> Returns all rows from the left DataFrame where there is a match in the right DataFrame.<br>\n",
    "<b>Left Anti Join:</b> Returns all rows from the left DataFrame where there is no match in the right DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd14b36",
   "metadata": {},
   "source": [
    "#### union\n",
    "It is used to combine the rows of two DataFrames with same schema\n",
    "unionAll was deprecated since PySpark 2.0.0 and union is used now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da61b9",
   "metadata": {},
   "source": [
    "#### unionByName\n",
    "This is used to merge two DataFrames by column names rather than position. Hence it can be used to merge two data frames where the column names are same but in different order. It also supports missing columns(Set allowMissingColumns to True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caabdc",
   "metadata": {},
   "source": [
    "#### udf - User Defined Functions\n",
    "It is used to register user functions so that they can be reused in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b932de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45399d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Converting function to UDF \n",
    "convertUDF = udf(lambda z: convertCase(z),StringType())\n",
    "\n",
    "#To register the function in sql\n",
    "#spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
    "\n",
    "#udf with annotation\n",
    "\n",
    "#@udf(returnType=StringType()) \n",
    "#def user_function():\n",
    "#    return something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff3ef51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|         Name|\n",
      "+-----+-------------+\n",
      "|    1|  John Jones |\n",
      "|    2|Tracey Smith |\n",
      "|    3| Amy Sanders |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.select(col(\"Seqno\"),convertUDF(col(\"Name\")).alias(\"Name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344704c5",
   "metadata": {},
   "source": [
    "#### DataFrame.transform()\n",
    "This function is used to apply multiple transformations on a data frame by chaining them and returns the final Data Frame.\n",
    "\n",
    "#### sql.functions.transform()\n",
    "This function is used to transform a column of ArrayType where the function takes another function as a paramter and applies to all the elements of the array. This is done to all the values of the column and new column is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3c556e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+--------------+\n",
      "|CourseName| fee|discount|new_fee|discounted_fee|\n",
      "+----------+----+--------+-------+--------------+\n",
      "|      Java|4000|       5|   3000|        2850.0|\n",
      "|    Python|4600|      10|   3600|        3240.0|\n",
      "|     Scala|4100|      15|   3100|        2635.0|\n",
      "|     Scala|4500|      15|   3500|        2975.0|\n",
      "|       PHP|3000|      20|   2000|        1600.0|\n",
      "+----------+----+--------+-------+--------------+\n",
      "\n",
      "+------------------+\n",
      "|        languages1|\n",
      "+------------------+\n",
      "|[JAVA, SCALA, C++]|\n",
      "|[SPARK, JAVA, C++]|\n",
      "|      [CSHARP, VB]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "def reduce_price(df,reduceBy=1000):\n",
    "    return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
    "\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discounted_fee\", df.new_fee - (df.new_fee * df.discount) / 100)\n",
    "\n",
    "# transform() \n",
    "transformedDF = df.transform(reduce_price) \\\n",
    "        .transform(apply_discount)\n",
    "                \n",
    "transformedDF.show()\n",
    "\n",
    "# Create DataFrame with Array\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
    "]\n",
    "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
    "\n",
    "\n",
    "# using sql.functions.transform() \n",
    "from pyspark.sql.functions import upper\n",
    "from pyspark.sql.functions import transform\n",
    "df.select(transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9483e",
   "metadata": {},
   "source": [
    "#### map\n",
    "It is an RDD transformation that is used to apply the transformation function (lambda) on every element of RDD/DataFrame and returns a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a48588bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+---+\n",
      "|         name|gender|age|\n",
      "+-------------+------+---+\n",
      "|Dinesh,Thakur|     M| 30|\n",
      "|   Karthick,K|     M| 26|\n",
      "|John,Williams|     M| 37|\n",
      "+-------------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Dinesh','Thakur','M',30),\n",
    "  ('Karthick','K','M',26),\n",
    "  ('John','Williams','M',37), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"age\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "\n",
    "#Accessing columns using indexes\n",
    "#map can only be perfromed on RDD and not DataFrame\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[0]+\",\"+x[1],x[2],x[3])\n",
    "    )  \n",
    "df2=rdd2.toDF([\"name\",\"gender\",\"age\"]   )\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e0218",
   "metadata": {},
   "source": [
    "#### flatMap\n",
    "Used to flatten the column of an RDD by passing a function to it which is applied to every element of the specified column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "814cab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\n",
      "Gutenbergâ€™s\n",
      "Aliceâ€™s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenbergâ€™s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenbergâ€™s\n"
     ]
    }
   ],
   "source": [
    "data = [\"Project Gutenbergâ€™s\",\n",
    "        \"Aliceâ€™s Adventures in Wonderland\",\n",
    "        \"Project Gutenbergâ€™s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenbergâ€™s\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2c5b6",
   "metadata": {},
   "source": [
    "#### foreach\n",
    "It takes a function as a parameter and applies it on every element of the DataFrame or RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff77b2",
   "metadata": {},
   "source": [
    "#### sample\n",
    "syntax - .sample(withReplacement, fraction, seed=None)\n",
    "It is used to get a sample of a dataset. Can be used on DataFrame and RDD\n",
    "\n",
    "#### sampleBy\n",
    "same syntax\n",
    "It is used to get stratified sample of data.\n",
    "\n",
    "#### takeSample\n",
    "same syntax\n",
    "Returns the sample as an array\n",
    "\n",
    "withReplacement is set to False by default. seed is used to generate same random sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f0c38",
   "metadata": {},
   "source": [
    "#### fill and fillna\n",
    "Both are aliases of each other and they are used to fill the null and none values with some other value that is given as a paramter to this function.\n",
    "syntax - .fill(value,subset=None)    The subset parameter is used to fill select subset of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a421b0",
   "metadata": {},
   "source": [
    "#### pivot\n",
    "It is used to rotate the values of a column to mulitple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4ee50ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n",
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5d0c0f",
   "metadata": {},
   "source": [
    "## PySpark SQL Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1de6df",
   "metadata": {},
   "source": [
    "### Aggregate functions\n",
    "\n",
    "approx_count_distinct<br>\n",
    "avg<br>\n",
    "collect_list<br>\n",
    "collect_set<br>\n",
    "countDistinct<br>\n",
    "count<br>\n",
    "grouping<br>\n",
    "first<br>\n",
    "last<br>\n",
    "kurtosis<br>\n",
    "max<br>\n",
    "min<br>\n",
    "mean<br>\n",
    "skewness<br>\n",
    "stddev<br>\n",
    "stddev_samp<br>\n",
    "stddev_pop<br>\n",
    "sum<br>\n",
    "sumDistinct<br>\n",
    "variance, var_samp, var_pop<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c8791",
   "metadata": {},
   "source": [
    "### Window functions\n",
    "\n",
    "<b>row_number()</b> gives the row number of the DataFrame row in the Window partition<br>\n",
    "<b>rank()</b> is used to provide a rank to the result within a Window partition<br>\n",
    "<b>dense_rank()</b> is same as rank but does not leave gaps in rank when there is a tie<br>\n",
    "<b>percent_rank()</b><br>\n",
    "<b>ntile()</b> returns relative rank of result rows in a Window partition<br>\n",
    "<b>cume_dist()</b> returns the cumulative distribution of values<br>\n",
    "<b>lead()</b> and <b>lag()</b> as in sql<br>\n",
    "aggregate functions such as sum, avg, etc..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8467dfd",
   "metadata": {},
   "source": [
    "### SQL Date and Timestamp functions\n",
    "\n",
    "#### Some date functions\n",
    "current_date()<br>\n",
    "date_format(dateExpr,format)<br>\n",
    "to_date()<br>\n",
    "to_date(column, fmt)<br>\n",
    "add_months(Column, numMonths)<br>\n",
    "date_add(column, days)<br>\n",
    "date_sub(column, days)<br>\n",
    "datediff(end, start)<br>\n",
    "months_between(end, start)<br>\n",
    "\n",
    "#### Some timestamp functions\n",
    "current_timestamp ()<br>\n",
    "hour(column)<br>\n",
    "minute(column)<br>\n",
    "second(column)<br>\n",
    "to_timestamp(column)<br>\n",
    "to_timestamp(column, format)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f31c15",
   "metadata": {},
   "source": [
    "### json functions\n",
    "\n",
    "from_json() - to convert json string to StructType or MapType<br>\n",
    "to_json() - to convert StructType or MapType to json string<br>\n",
    "json_tuple() - to extract the columns from json and generate new columns from the result\n",
    "get_json_object() - is used to extract the json string based on path from the json column\n",
    "schema_of_json() - to create schema string from json string column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8017765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n",
      "using from_json\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|id |value                                                                      |\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n",
      "+---+---------------------------------------------------------------------------+\n",
      "\n",
      "using to_json\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|id |value                                                                       |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+----------------------------------------------------------------------------+\n",
      "\n",
      "using json_tuple\n",
      "+---+-------+-----------+-----------+\n",
      "|id |Zipcode|ZipCodeType|City       |\n",
      "+---+-------+-----------+-----------+\n",
      "|1  |704    |STANDARD   |PARC PARQUE|\n",
      "+---+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType,StringType\n",
    "from pyspark.sql.functions import from_json, to_json, json_tuple\n",
    "\n",
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
    "df.show(truncate=False)\n",
    "\n",
    "print(\"using from_json\")\n",
    "df2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "print(\"using to_json\")\n",
    "df2.withColumn(\"value\",to_json(col(\"value\"))) \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "print(\"using json_tuple\")\n",
    "df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n",
    "    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094729e2",
   "metadata": {},
   "source": [
    "## PySpark Data sources\n",
    "\n",
    "###  Read and Write csv files\n",
    "df = spark.read.csv(\"filename\") - to read the csv file<br>\n",
    "df = spark.read.option(\"header\", True).csv(\"filename\") - set to true if the first row is a row of headers<br>\n",
    "df = df = spark.read.csv(\"path1,path2,path3\") - read multiple csv files\n",
    "df = spark.read.csv(\"folder_path\") - read all csv files in a folder\n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "  .csv(\"/tmp/resources/zipcodes.csv\") - setting multiple options at once<br>\n",
    "Note - inferSchema is used to infer the column types from the values of the csv file and delimiter is the one which separates columns<br>\n",
    "\n",
    "df_with_schema = spark.read.format(\"csv\").option(\"header\", True).schema(user_schema).load(\"/tmp/resources/zipcodes.csv\") - reading csv file based on user's schema<br>\n",
    "\n",
    "df2.write.options(header='True', delimiter=',').csv(\"/tmp/spark_output/zipcodes\" - writing the data frame to a csv file<br>\n",
    "\n",
    "modes - overwrite, append, ignore - if file already exists, error - error if file already exists<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5a6a6",
   "metadata": {},
   "source": [
    "### Read and Write parquet files\n",
    "\n",
    "Same as for csv files. Replace csv with parquet\n",
    "\n",
    "Creating a table on Partuet file -<br>\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"/tmp/output/people.parquet\\\")\")<br>\n",
    "spark.sql(\"SELECT * FROM PERSON\").show()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cba1b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "| Banana|  1000|    USA|\n",
      "|Carrots|  1500|    USA|\n",
      "|  Beans|  1600|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Banana|   400|  China|\n",
      "|Carrots|  1200|  China|\n",
      "|  Beans|  1500|  China|\n",
      "| Orange|  4000|  China|\n",
      "| Banana|  2000| Canada|\n",
      "|Carrots|  2000| Canada|\n",
      "|  Beans|  2000| Mexico|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.write.mode('overwrite').parquet(\"sample.parquet\")\n",
    "spark.sql(\"CREATE TEMPORARY VIEW SAMPLE USING parquet OPTIONS (path \\\"sample.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM SAMPLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab5659",
   "metadata": {},
   "source": [
    "### Read and Write JSON file\n",
    "Same as for parquet. Replace parquet with json. You can also create tables on json files in a similar way.<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165d539",
   "metadata": {},
   "source": [
    "### Reading a Hive table\n",
    "<b>Enable hive support while creating spark session</b><br>\n",
    "warehouse_location = abspath('spark-warehouse')<br>\n",
    "spark = SparkSession \\ <br>\n",
    "    .builder \\ <br>\n",
    "    .appName(\"SparkByExamples.com\") \\ <br>\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\ <br>\n",
    "    .enableHiveSupport() \\ <br>\n",
    "    .getOrCreate()<br>\n",
    "    \n",
    "\n",
    "To read a hive table -<br>\n",
    "df = spark.sql(\"select * from emp.employee\") or df = spark.read.table(\"employee\")\n",
    "df.show()<br><br><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d25eb6",
   "metadata": {},
   "source": [
    "### Save DataFrame to Hive table\n",
    "\n",
    "#### Creating a Hive Internal Table -<br>\n",
    "df.write.mode('overwrite').saveAsTable(\"table_name\")<br>\n",
    "\n",
    "#### Creating a Hive Internal Table inside a database<br>\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS database_name\")<br>\n",
    "df.write.mode('overwrite').saveAsTable(\"database_name.table_name\")<br>\n",
    "\n",
    "#### Creating a Hive External Table\n",
    "sampleDF.write.mode(\"overwrite\").option(\"path\", \"/path/to/external/table\").saveAsTable(\"database_name.table_name\")<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f74c0",
   "metadata": {},
   "source": [
    "### Read JDBC in parallel\n",
    "\n",
    "#### Reading JDBC in parallel(using necessary config)\n",
    "spark = SparkSession.builder \\ <br>\n",
    "           .appName('SparkByExamples.com') \\ <br>\n",
    "           .config(\"spark.jars\", \"mysql-connector-java-8.0.13.jar\") \\ <br>\n",
    "           .getOrCreate()<br>\n",
    "<br>\n",
    "df = spark.read \\ <br>\n",
    "    .jdbc(\"jdbc:mysql://localhost:3306/emp\", \"employee\", \\ <br>\n",
    "          properties={\"user\": \"root\", \"password\": \"root\", \"driver\":\"com.mysql.cj.jdbc.Driver\"})<br><br>\n",
    "\n",
    "df.show()<br><br>\n",
    "\n",
    "#### Read Table in Parallel\n",
    "df = spark.read \\ <br>\n",
    "    .format(\"jdbc\") \\ <br>\n",
    "    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\ <br>\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/emp\") \\ <br>\n",
    "    .option(\"dbtable\",\"employee\") \\ <br>\n",
    "    .option(\"numPartitions\",5) \\ <br>\n",
    "    .option(\"user\", \"root\") \\ <br>\n",
    "    .option(\"password\", \"root\") \\ <br>\n",
    "    .load() <br><br>\n",
    "    \n",
    "#### Select columns with where clause (You can use either dbtable or query not both)\n",
    "df = spark.read \\ <br>\n",
    "    .format(\"jdbc\") \\ <br>\n",
    "    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\ <br>\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/emp\") \\ <br>\n",
    "    .option(\"query\",\"select id,age from employee where gender='M'\") \\ <br>\n",
    "    .option(\"numPartitions\",5) \\ <br>\n",
    "    .option(\"user\", \"root\") \\ <br>\n",
    "    .option(\"password\", \"root\") \\ <br>\n",
    "    .load() <br><br>\n",
    "    \n",
    "#### Using fetchsize(No. of rows to fetch - default: 10)\n",
    "df = spark.read \\ <br>\n",
    "    .format(\"jdbc\") \\ <br>\n",
    "    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\ <br>\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/emp\") \\ <br>\n",
    "    .option(\"query\",\"select id,age from employee where gender='M'\") \\ <br>\n",
    "    .option(\"numPartitions\",5) \\ <br> \n",
    "    .option(\"fetchsize\", 20) \\  <br>\n",
    "    .option(\"user\", \"root\") \\ <br>\n",
    "    .option(\"password\", \"root\") \\ <br>\n",
    "    .load() <br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e09f67",
   "metadata": {},
   "source": [
    "### Read and Write to SQL server\n",
    "\n",
    "#### Writing data frame to SQL server table\n",
    "df.write \\ <br>\n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \\ <br>\n",
    "  .mode(\"overwrite\") \\ <br>\n",
    "  .option(\"url\", \"jdbc:sqlserver://{SERVER_ADDR};databaseName=emp;\") \\ <br>\n",
    "  .option(\"dbtable\", \"employee\") \\ <br>\n",
    "  .option(\"user\", \"replace_user_name\") \\ <br>\n",
    "  .option(\"password\", \"replace_password\") \\ <br>\n",
    "  .save() <br>\n",
    "  \n",
    "#### Reading data from SQL server table\n",
    "df = spark.read \\ <br>\n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \\ <br>\n",
    "  .option(\"url\", \"jdbc:sqlserver://{SERVER_ADDR};databaseName=emp;\") \\ <br>\n",
    "  .option(\"dbtable\", \"employee\") \\ <br>\n",
    "  .option(\"user\", \"replace_user_name\") \\ <br>\n",
    "  .option(\"password\", \"replace_password\") \\ <br>\n",
    "  .load() <br>\n",
    "  \n",
    "#### Selecting specific columns to read\n",
    "df = spark.read \\ <br>\n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \\ <br>\n",
    "  .option(\"url\", \"jdbc:sqlserver://{SERVER_ADDR};databaseName=emp;\") \\ <br>\n",
    "  .option(\"dbtable\", \"select id,age from employee where gender='M'\") \\ <br>\n",
    "  .option(\"user\", \"replace_user_name\") \\ <br>\n",
    "  .option(\"password\", \"replace_password\") \\ <br>\n",
    "  .load() <br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e3f9b",
   "metadata": {},
   "source": [
    "### Read and Write to Mysql\n",
    "\n",
    "spark = SparkSession.builder \\ <br>\n",
    "           .appName('SparkByExamples.com') \\ <br>\n",
    "           .config(\"spark.jars\", \"mysql-connector-java-8.0.13.jar\") <br>\n",
    "           .getOrCreate() <br>\n",
    "           \n",
    "#### Writing to mysql table\n",
    "df.write \\ <br>\n",
    "  .format(\"jdbc\") \\ <br>\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\ <br>\n",
    "  .option(\"url\", \"jdbc:mysql://localhost:3306/emp\") \\ <br>\n",
    "  .option(\"dbtable\", \"employee\") \\ <br>\n",
    "  .option(\"user\", \"root\") \\ <br>\n",
    "  .option(\"password\", \"root\") \\ <br>\n",
    "  .save() <br>\n",
    "  \n",
    "#### Reading from mysql table\n",
    "df = spark.read \\ <br>\n",
    "    .format(\"jdbc\") \\ <br>\n",
    "    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\ <br>\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/emp\") \\ <br>\n",
    "    .option(\"dbtable\", \"employee\") \\ <br>\n",
    "    .option(\"user\", \"root\") \\ <br>\n",
    "    .option(\"password\", \"root\") \\ <br>\n",
    "    .load() <br> <br>\n",
    "    \n",
    "    Note - You can use a query instead of table name to get specific columns\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bface9",
   "metadata": {},
   "source": [
    "### Read JDBC Table\n",
    "\n",
    "#### Read from MySQL Table\n",
    "df = spark.read \\ <br>\n",
    "    .format(\"jdbc\") \\ <br>\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/emp\") \\ <br>\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\ <br>\n",
    "    .option(\"dbtable\", \"employee\") \\ <br>\n",
    "    .option(\"user\", \"root\") \\ <br>\n",
    "    .option(\"password\", \"root\") \\ <br>\n",
    "    .load() <br><br>\n",
    "    \n",
    "#### Read from MySQL Table\n",
    "df = spark.read \\ <br>\n",
    "    .format(\"jdbc\") \\ <br>\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/emp\") \\ <br>\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\ <br>\n",
    "    .option(\"dbtable\", \"select id,age from employee where gender='M'\") \\ <br>\n",
    "    .option(\"user\", \"root\") \\ <br>\n",
    "    .option(\"password\", \"root\") \\ <br>\n",
    "    .load() <br><br>\n",
    "\n",
    "df.show() <br><br>\n",
    "\n",
    "#### Using numPartitions\n",
    "df = spark.read \\ <br>\n",
    "  .format(\"jdbc\") \\ <br>\n",
    "  .option(\"query\", \"select id,age from employee where gender='M'\") \\ <br>\n",
    "  .option(\"numPartitions\",5) \\ <br>\n",
    "  .option(\"fetchsize\", 20) \\ <br>\n",
    "  ....... <br>\n",
    "  .load() <br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b38e6",
   "metadata": {},
   "source": [
    "## Some built in functions of PySpark\n",
    "\n",
    "<b>expr</b> - used to execute sql-like expressions<br>\n",
    "<b>lit</b> - used to add columns with literals or constant values. Returns Column type<br>\n",
    "<b>split</b> - splits a string into an array of strings based on a specified delimiter<br>\n",
    "<b>concat_ws</b> - combines the elements of an array using a delimiter for concatenation<br>\n",
    "<b>substring(str, pos, len)</b> - used to get substring of a string with position and length as parameters<br>\n",
    "<b>regexp_replace</b> - used to replace a value with another in a column<br>\n",
    "<b>translate</b> - same as regexp_replace but replaces character by character<br>\n",
    "<b>overlay</b> - used to replace a column value with string of another column from a specified position<br>\n",
    "<b>collect_list(), collect_set() </b>- used to collect column values as a list, set and map respectively<br>\n",
    "<b>create_map</b> - used to create a map<br>\n",
    "<b>struct</b> - used to change the structure of existing dataframe and add new StructType to it<br>\n",
    "<b>countDistinct</b> - used to get count of distinct values of a group<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc163f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
